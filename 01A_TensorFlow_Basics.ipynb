{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If running this notebook directly, make sure you are running your Jupyter kernel in an environment with TensorFlow installed. Some useful packages to install/import first, if you didn't in notebook 00A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .........\n",
      "Solving package specifications: .\n",
      "\n",
      "# All requested packages already installed.\n",
      "# packages in environment at /Users/lz54/anaconda3/envs/tensorflow:\n",
      "#\n",
      "matplotlib                2.0.2               np113py36_0  \n",
      "Requirement already up-to-date: tqdm in /Users/lz54/anaconda3/envs/tensorflow/lib/python3.6/site-packages\n"
     ]
    }
   ],
   "source": [
    "# Beginning a line with \"!\" allows you to execute a command in your terminal\n",
    "# You'll need to install these packages for certain lines of this notebook if you don't already have them.\n",
    "!conda install -y matplotlib\n",
    "!pip install -U tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #matlab like lib\n",
    "from tqdm import trange #some bar/ slider lib         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic TensorFlow test from the installation instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple TensorFlow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The example from the TensorFlow Introduction slides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0, dtype=tf.float32)\n",
    "sum_a_b = tf.add(a, b)\n",
    "\n",
    "# Using the Python \"with\" as a context manager\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(sum_a_b)) # Prints “7.0” to the screen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous graph only produces a constant output. Not particularly interesting. To create a similar graph that can acccept variable inputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Result: 7.0\n",
      "2nd Result: 4035.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "sum_a_b = tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed_dict = {a: 3.0, b: 4.0}\n",
    "    print('1st Result: {0}'.format(sess.run(sum_a_b, feed_dict=feed_dict)))\n",
    "    \n",
    "    feed_dict = {a: 2015, b: 2020}\n",
    "    print('2nd Result: {0}'.format(sess.run(sum_a_b, feed_dict=feed_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is very popular machine learning dataset, consisting of 70000 grayscale images of handwritten digits, of dimensions 28x28. We'll be using it as our example for this section of the tutorial, with the goal being to predict which the digit is in each image.\n",
    "\n",
    "![mnist](Figures/mnist.PNG)\n",
    "\n",
    "Since it's such a common (and small) dataset, TensorFlow has commands for downloading and formatting the dataset conveniently baked in already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the data is organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image data: (55000, 784)\n",
      "Validation image data: (5000, 784)\n",
      "Testing image data: (10000, 784)\n",
      "28 x 28 = 784\n",
      "\n",
      "Test Labels: (10000, 10)\n",
      "Label distribution:[(0, 980), (1, 1135), (2, 1032), (3, 1010), (4, 982), (5, 892), (6, 958), (7, 1028), (8, 974), (9, 1009)]\n",
      "\n",
      "Train image 1 is labelled one-hot as [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e8803f1eb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e8fcd37f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset statistics\n",
    "print('Training image data: {0}'.format(mnist.train.images.shape))\n",
    "print('Validation image data: {0}'.format(mnist.validation.images.shape))\n",
    "print('Testing image data: {0}'.format(mnist.test.images.shape))\n",
    "print('28 x 28 = {0}'.format(28*28))\n",
    "\n",
    "print('\\nTest Labels: {0}'.format(mnist.test.labels.shape))\n",
    "labels = np.arange(10)\n",
    "num_labels = np.sum(mnist.test.labels, axis=0, dtype=np.int)\n",
    "print('Label distribution:{0}'.format(list(zip(labels, num_labels))))\n",
    "\n",
    "# Example image\n",
    "print('\\nTrain image 1 is labelled one-hot as {0}'.format(mnist.train.labels[1,:]))\n",
    "image = np.reshape(mnist.train.images[1,:],[28,28])\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the graph input: this is where we feed in our training images into the model. Since MNIST digits are pretty small and the model we're using is very simple, we'll feed them in as flat vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our predicted probabilities of each digit, let's first start with the probability of a digit being a 3 like the image above. For our simple model, we start by applying a linear transformation. That is, we multiply each value of the input vector by a weight, sum them all together, and then add a bias. In equation form:\n",
    "\n",
    "\\begin{align}\n",
    "y_3 = \\sum_i w_{i,3} x_i + b_3\n",
    "\\end{align}\n",
    "\n",
    "The magnitude of this result $y_3$, we'll take as being correlated to our belief in how likely we think the input digit was a 3. The higher the value of $y_3$, the more likely we think the input image $x$ was a 3 (ie, we'd hope we'd get a relatively large value for $y_3$ for the above image). Remember though, our original goal was to identify all 10 digits, so we also have:\n",
    "\n",
    "\\begin{align*}\n",
    "y_0 =& \\sum_i w_{i,0} x_i + b_0 \\\\\n",
    "&\\vdots \\\\\n",
    "y_9 =& \\sum_i w_{i,9} x_i + b_9\n",
    "\\end{align*}\n",
    "\n",
    "We can express this in matrix form as:\n",
    "\n",
    "\\begin{align}\n",
    "y = W x + b \n",
    "\\end{align}\n",
    "\n",
    "To put this into our graph in TensorFlow, we need to define some Variables to hold the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define linear transformation\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret these values (aka logits) $y$ as probabilities if we normalize them to be positive and add up to 1. In logistic regression, we do this with a softmax:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_i) = \\text{softmax}(y_i) = \\frac{\\text{exp}(y_i)}{\\sum_j\\text{exp}(y_j)}\n",
    "\\end{align}\n",
    "\n",
    "Notice that because the range of the exponential function is always non-negative, and since we're normalizing by the sum, the softmax achieves the desired property of producing values between 0 and 1 that sum to 1. If we look at the case with only 2 classes, we see that the softmax is the multi-class extension of the binary sigmoid function: \n",
    "<img src=\"Figures/Logistic-curve.png\", width=\"500\">\n",
    "\n",
    "Computing a softmax in TensorFlow is pretty easy, sort of*:\n",
    "\n",
    "*&#42;More on this later*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax to probabilities\n",
    "py = tf.nn.softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That defines our forward pass of our model! We now have a graph that performs a forward pass: given an input image, the graph returns the probabilities the model thinks the input is each of the 10 classes. Are we done?\n",
    "\n",
    "Not quite. We don't know the values of $W$ and $b$ yet. We're going to learn those by defining a loss and using gradient descent to do backpropagation. Essentially, we'll be taking the derivative with respect to each of the elements in $W$ and $b$ and wiggling them in a direction that reduces our loss.\n",
    "\n",
    "The loss we commonly use in classification is cross-entropy. Cross-entropy is a concept from information theory:\n",
    "\n",
    "\\begin{align}\n",
    "H_{y'}(y)=-\\sum_i y'_i \\text{log}(y_i)\n",
    "\\end{align}\n",
    "\n",
    "Cross-entropy not only captures how *correct* (max probability corresponds to the right answer) the model's answers are, it also accounts for how *confident* (high confidence in correct answers) they are. This encourages the model to produce very high probabilities for correct answers while driving down the probabilities for the wrong answers, instead of merely be satisfied with it being the argmax. \n",
    "\n",
    "In supervised models, we need labels to learn, so we create a placeholder for the labels in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define labels placeholder\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy loss is pretty easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(py), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the old days, we would have to go through and derive all the gradients ourselves, then code them into our program. Nowadays, we have libraries to compute all the gradients automatically. Not only that, but TensorFlow comes with a whole suite of optimizers implementing various optimization algorithms. I'm not going to go into the details of why you should appreciate that right now, because I know that Prof David Carlson has an entire day's worth of material on optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, we simply call the optimizer op we defined above. First though, we need to start a session and initialize our variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a session object and initialize all graph variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are much cleverer ways to design a training regimen that stop training once the model is converged and before it starts overfitting, but for this demo, we'll keep it simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:01<00:00, 571.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# trange is a tqdm function. It's the same as range, but adds a pretty progress bar\n",
    "for _ in trange(1000): \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, because of the way the dependency links are connected in our graph, running the optimizer requires an input to both the training image placeholder `x` and the training label placeholder `y_` (as it should). The values of all variables (`W` and `b`) are updated in place automatically by the optimizer.\n",
    "\n",
    "Now let's see how we did! For every image in our test set, we run the data through the model, and take the digit in which we have the highest confidence as our answer. We then compute an accuracy by seeing how many we got correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.902899980545044\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(py, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a simple model and a few lines of code.  Before we close the session, there's one more interesting thing we can do. Normally, it can be difficult to inspect exactly what the filters in a model are doing, but since this model is so simple, and the weights transform the data directly to their logits, we can actually visualize what the model's learning by simply plotting the weights. The results look pretty reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d1ffe5fd2cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "# Get weights\n",
    "weights = sess.run(W)\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 2))\n",
    "\n",
    "for digit in range(10):\n",
    "    ax[digit].imshow(weights[:,digit].reshape(28,28), cmap='gray')\n",
    "\n",
    "# Close session to finish\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire model, with the complete model definition, training, and evaluation (but minus the weights visualization), is below. Note the slight difference when calculating the softmax; this is done for numerical stability purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 669.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9162999987602234\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import trange\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt #matlab like lib\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Create a Session object, initialize all variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train\n",
    "for _ in trange(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "\n",
    "# Get weights\n",
    "weights = sess.run(W)\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 2))\n",
    "\n",
    "for digit in range(10):\n",
    "    ax[digit].imshow(weights[:,digit].reshape(28,28), cmap='gray')\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The accuracy from the full version directly above might return a slightly different test accuracy from the step-by-step version we first went through. This is because mnist.train.next_batch by default shuffles the order of the training data, so we're seeing the data in a different order.\n",
    "\n",
    "*Acknowledgment: Material adapted from the TensorFlow tutorial: https://www.tensorflow.org/get_started/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
